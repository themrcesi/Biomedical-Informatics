{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Prerequisites\" data-toc-modified-id=\"Prerequisites-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prerequisites</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#File-paths\" data-toc-modified-id=\"File-paths-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>File paths</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creación-del-Diccionario\" data-toc-modified-id=\"Creación-del-Diccionario-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Creación del Diccionario</a></span></li><li><span><a href=\"#BOW-generation\" data-toc-modified-id=\"BOW-generation-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>BOW generation</a></span></li><li><span><a href=\"#Similarity-matrix-generation\" data-toc-modified-id=\"Similarity-matrix-generation-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Similarity matrix generation</a></span></li></ul></li><li><span><a href=\"#Data-load\" data-toc-modified-id=\"Data-load-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data load</a></span><ul class=\"toc-item\"><li><span><a href=\"#Titles\" data-toc-modified-id=\"Titles-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Titles</a></span></li><li><span><a href=\"#Judgements\" data-toc-modified-id=\"Judgements-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Judgements</a></span></li><li><span><a href=\"#Queries\" data-toc-modified-id=\"Queries-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Queries</a></span></li><li><span><a href=\"#Metadata\" data-toc-modified-id=\"Metadata-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Metadata</a></span></li></ul></li><li><span><a href=\"#Retrieval-models\" data-toc-modified-id=\"Retrieval-models-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Retrieval models</a></span><ul class=\"toc-item\"><li><span><a href=\"#TFIDF-Model\" data-toc-modified-id=\"TFIDF-Model-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>TFIDF Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-of-evaluation\" data-toc-modified-id=\"Example-of-evaluation-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Example of evaluation</a></span></li><li><span><a href=\"#Parameterized-Evaluation\" data-toc-modified-id=\"Parameterized-Evaluation-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Parameterized Evaluation</a></span></li></ul></li><li><span><a href=\"#TFIDF-2.0\" data-toc-modified-id=\"TFIDF-2.0-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>TFIDF 2.0</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creation\" data-toc-modified-id=\"Creation-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;</span>Creation</a></span></li><li><span><a href=\"#Saving\" data-toc-modified-id=\"Saving-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;</span>Saving</a></span></li><li><span><a href=\"#Loading\" data-toc-modified-id=\"Loading-6.3.3\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;</span>Loading</a></span></li><li><span><a href=\"#Getting-embedding\" data-toc-modified-id=\"Getting-embedding-6.3.4\"><span class=\"toc-item-num\">6.3.4&nbsp;&nbsp;</span>Getting embedding</a></span></li><li><span><a href=\"#Queries-embeddings\" data-toc-modified-id=\"Queries-embeddings-6.3.5\"><span class=\"toc-item-num\">6.3.5&nbsp;&nbsp;</span>Queries embeddings</a></span></li><li><span><a href=\"#Documents-embeddings\" data-toc-modified-id=\"Documents-embeddings-6.3.6\"><span class=\"toc-item-num\">6.3.6&nbsp;&nbsp;</span>Documents embeddings</a></span></li><li><span><a href=\"#Launch-query-in-W2V-Model\" data-toc-modified-id=\"Launch-query-in-W2V-Model-6.3.7\"><span class=\"toc-item-num\">6.3.7&nbsp;&nbsp;</span>Launch query in W2V Model</a></span></li></ul></li><li><span><a href=\"#Fast-text\" data-toc-modified-id=\"Fast-text-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Fast text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creation\" data-toc-modified-id=\"Creation-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;</span>Creation</a></span></li><li><span><a href=\"#Saving\" data-toc-modified-id=\"Saving-6.4.2\"><span class=\"toc-item-num\">6.4.2&nbsp;&nbsp;</span>Saving</a></span></li><li><span><a href=\"#Loading\" data-toc-modified-id=\"Loading-6.4.3\"><span class=\"toc-item-num\">6.4.3&nbsp;&nbsp;</span>Loading</a></span></li><li><span><a href=\"#Getting-embeddings\" data-toc-modified-id=\"Getting-embeddings-6.4.4\"><span class=\"toc-item-num\">6.4.4&nbsp;&nbsp;</span>Getting embeddings</a></span></li><li><span><a href=\"#Queries-embeddings\" data-toc-modified-id=\"Queries-embeddings-6.4.5\"><span class=\"toc-item-num\">6.4.5&nbsp;&nbsp;</span>Queries embeddings</a></span></li><li><span><a href=\"#Documents-embeddings\" data-toc-modified-id=\"Documents-embeddings-6.4.6\"><span class=\"toc-item-num\">6.4.6&nbsp;&nbsp;</span>Documents embeddings</a></span></li><li><span><a href=\"#Launch-query-in-FastText-Model\" data-toc-modified-id=\"Launch-query-in-FastText-Model-6.4.7\"><span class=\"toc-item-num\">6.4.7&nbsp;&nbsp;</span>Launch query in FastText Model</a></span></li></ul></li><li><span><a href=\"#Latent-Semantic-Indexing\" data-toc-modified-id=\"Latent-Semantic-Indexing-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Latent Semantic Indexing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-creation\" data-toc-modified-id=\"Model-creation-6.5.1\"><span class=\"toc-item-num\">6.5.1&nbsp;&nbsp;</span>Model creation</a></span></li><li><span><a href=\"#Similarities-matrix\" data-toc-modified-id=\"Similarities-matrix-6.5.2\"><span class=\"toc-item-num\">6.5.2&nbsp;&nbsp;</span>Similarities matrix</a></span></li><li><span><a href=\"#Launching-queries\" data-toc-modified-id=\"Launching-queries-6.5.3\"><span class=\"toc-item-num\">6.5.3&nbsp;&nbsp;</span>Launching queries</a></span></li></ul></li></ul></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Results</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Conclusions</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Make sure you have downloaded the following pakcages in your environment:\n",
    "\n",
    "- nltk: nltk.download('stopwords')\n",
    "- xmltodict\n",
    "- numpy\n",
    "- gensim\n",
    "- pandas\n",
    "- matplotlib\n",
    "- dask\n",
    "- sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:55:07.037943Z",
     "start_time": "2020-12-13T19:54:55.821985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.5-py3-none-any.whl\n",
      "Requirement already satisfied: click in c:\\users\\david.rubio\\appdata\\roaming\\python\\python38\\site-packages (from nltk) (7.1.2)\n",
      "Collecting joblib\n",
      "  Using cached joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "Collecting regex\n",
      "  Using cached regex-2020.11.13-cp38-cp38-win_amd64.whl (270 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.54.1-py2.py3-none-any.whl (69 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-0.17.0 nltk-3.5 regex-2020.11.13 tqdm-4.54.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:55:09.271929Z",
     "start_time": "2020-12-13T19:55:07.042196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Using cached xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:55:20.868888Z",
     "start_time": "2020-12-13T19:55:09.273750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.19.3\n",
      "  Using cached numpy-1.19.3-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.19.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\David.Rubio\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ray 1.0.1.post1 requires pyyaml, which is not installed.\n",
      "ray 1.0.1.post1 requires requests, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install --user numpy==1.19.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:55:46.102322Z",
     "start_time": "2020-12-13T19:55:20.870874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-3.8.3-cp38-cp38-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\david.rubio\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (1.19.3)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\david.rubio\\anaconda3\\envs\\test2\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Collecting Cython==0.29.14\n",
      "  Using cached Cython-0.29.14-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Collecting scipy>=0.18.1\n",
      "  Using cached scipy-1.5.4-cp38-cp38-win_amd64.whl (31.4 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\david.rubio\\appdata\\roaming\\python\\python38\\site-packages (from gensim) (1.19.3)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-4.0.1-py3-none-any.whl\n",
      "Installing collected packages: smart-open, scipy, Cython, gensim\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 scipy-1.5.4 smart-open-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:56:03.007896Z",
     "start_time": "2020-12-13T19:55:46.105581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.1.5-cp38-cp38-win_amd64.whl (9.0 MB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\david.rubio\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (1.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\david.rubio\\anaconda3\\envs\\test2\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\david.rubio\\anaconda3\\envs\\test2\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.23.0 requires requests<3.0.0dev,>=2.18.0, which is not installed.\n",
      "modin 0.8.2 requires pandas==1.1.4, but you have pandas 1.1.5 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.5 pytz-2020.4\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:56:13.268304Z",
     "start_time": "2020-12-13T19:56:03.010312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.3.3-cp38-cp38-win_amd64.whl (8.5 MB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\david.rubio\\anaconda3\\envs\\test2\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\david.rubio\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (1.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\david.rubio\\anaconda3\\envs\\test2\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six in c:\\users\\david.rubio\\anaconda3\\envs\\test2\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.3.1-cp38-cp38-win_amd64.whl (51 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-8.0.1-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: six in c:\\users\\david.rubio\\anaconda3\\envs\\test2\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Installing collected packages: pillow, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.3 pillow-8.0.1\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:56:21.053600Z",
     "start_time": "2020-12-13T19:56:13.271296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "  Using cached dask-2020.12.0-py3-none-any.whl (884 kB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-5.3.1-cp38-cp38-win_amd64.whl (219 kB)\n",
      "Installing collected packages: pyyaml, dask\n",
      "Successfully installed dask-2020.12.0 pyyaml-5.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ray 1.0.1.post1 requires requests, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:56:31.956363Z",
     "start_time": "2020-12-13T19:56:21.059477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-0.23.2-cp38-cp38-win_amd64.whl (6.8 MB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\david.rubio\\anaconda3\\envs\\test2\\lib\\site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\david.rubio\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn->sklearn) (1.19.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\david.rubio\\anaconda3\\envs\\test2\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\david.rubio\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn->sklearn) (1.19.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.23.2 sklearn-0.0 threadpoolctl-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:56:34.463070Z",
     "start_time": "2020-12-13T19:56:31.957700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\David.Rubio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Import the required libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:56:35.375232Z",
     "start_time": "2020-12-13T19:56:34.466086Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import xmltodict as xtd\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from joblib import Parallel, delayed\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import pandas as pd\n",
    "from gensim.similarities import MatrixSimilarity, SparseMatrixSimilarity, Similarity\n",
    "from operator import itemgetter\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File paths\n",
    "\n",
    "Add the paths for the queries file, the texts (documents) and the judements file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T19:56:35.391634Z",
     "start_time": "2020-12-13T19:56:35.378243Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"path_queries = r\"B:\\document_parser\\document_parses\\topics-rnd5.xml\"\n",
    "path_texts = \"B:\\document_parser\\document_parses\\pdf_json\"\n",
    "path_test = \"B:/document_parser/document_parses/test\"\n",
    "path_judgements = \"B:/document_parser/document_parses/judgements.csv\"\n",
    "path_judgements2 = \"B:/document_parser/document_parses/judgements2.csv\"\n",
    "\"\"\"\n",
    "path_queries = r\"C:/Users/David.Rubio/Desktop/muia/topics-rnd5.xml\"\n",
    "path_texts = \"C:/Users/David.Rubio/Desktop/muia/2020-07-16/document_parses/pdf_json\"\n",
    "path_judgements = \"C:/Users/David.Rubio/Desktop/muia/qrels-covid_d5_j0.5-5.txt\"\n",
    "path_judgements2 = \"C:/Users/David.Rubio/Desktop/muia/qrels-covid_d5_j0.5-5.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this section, the dictionary, the bag of words (bow) and the similarity matrix will be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary generation\n",
    "\n",
    "Regarding the dictionary, we found out that there exists another special dictionary called HashDictionary, that doesn't need to be filled before used it. However, it is likely that two different words get the same key. So, we decided to use the basic one in spite of having to fill it before hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.858Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2-5 minutes\n",
    "dictionary = corpora.Dictionary(line.split() for line in open(\"docs.txt\",\"r\"))\n",
    "dictionary.save('covid19.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW generation\n",
    "\n",
    "First, the stopwords list needs to be assigned to a variable. This list contains the most common words in a language (e.g. 'in', 'the', 'a'...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.862Z"
    }
   },
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method *preprocess_document* processes a document given a stopword list. In this method, we concatenate the information given by the title, the abstract and the body of he document. These data are the most relevant for making a ranking given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.865Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_document(doc,stopset):\n",
    "    title = np.array([doc[\"metadata\"][\"title\"]], dtype=str)\n",
    "    abstract = np.array([paragraph[\"text\"] for paragraph in doc[\"abstract\"]], dtype=str)\n",
    "    text = np.array([paragraph[\"text\"] for paragraph in doc[\"body_text\"]], dtype=str)\n",
    "    stemmer = PorterStemmer()\n",
    "    information = np.concatenate((title, abstract, text))\n",
    "    tokens = np.concatenate(([wordpunct_tokenize(inf) for inf in information]))\n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2 and not token.isnumeric()]\n",
    "    final = \" \".join([stemmer.stem(word) for word in clean])\n",
    "    return title[0], str(final.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data each file in path_texts we write one row per document in docs.txt with the most relevant information (use preprocess_document). This information will be essential to build the dictionary. This step takes more than an hour so, unless you need to generate the file, we strongly suggest not to remove the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.880Z"
    }
   },
   "outputs": [],
   "source": [
    "# More than 1 hour\n",
    "files = glob.glob(path_texts + \"/*.json\")\n",
    "with open(\"docs.txt\", \"w+\") as f:\n",
    "    i = 1\n",
    "    for file in files:\n",
    "        print(i)\n",
    "        i+=1\n",
    "        with open(file) as js:\n",
    "            file_json = json.load(js)\n",
    "        _, stems = preprocess_document(file_json, stopset)\n",
    "        f.write(stems.strip('b\\'')+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class MyCorpus will help to iterate (\\_\\_iter\\_\\_ method) over each line of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.886Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        self.path = file\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in open(self.path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over each row in \"docs.txt\" to build the bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.891Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2-5 minutes\n",
    "path_corpus = \"docs.txt\"\n",
    "bow = MyCorpus(path_corpus)\n",
    "corpora.MmCorpus.serialize(\"covid19.mm\", bow, metadata=True)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity matrix generation\n",
    "\n",
    "Regarding the similarities, we chose to use Similarity instead of MatrixSimilarity since it allows lazy generation, not making us to store all the data in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.896Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3-5 minutes\n",
    "index = Similarity(None, corpus = bow, num_features=len(dictionary))  # create index\n",
    "index.save(\"covid19.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load \n",
    "\n",
    "In this section, relevant information will be loaded locally (title, judgmenets...). This information is useful during the evaluation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titles\n",
    "\n",
    "The title and the paper_id for each document in \"path_texts\" are needed to link the paper_id retrieved by the model with the title.\n",
    "\n",
    "For this task, dask is used to reduce the computing time (parallel computing): 6 workers with 2 threads per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.899Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "from dask.distributed import Client, progress\n",
    "client = Client(n_workers=6, threads_per_worker=2)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method load_dataset calls load_json for each file in the path (document). For this document, load_json calls load_dataset to get the paper_id, the title, the abstract and the body of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.902Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    files = glob.glob(path + \"/*.json\")\n",
    "    b = db.from_sequence(files).map(load_json)\n",
    "    df = b.to_dataframe(columns=[\"id\", \"title\", \"abstract\", \"body\"])\n",
    "    return df\n",
    "\n",
    "def load_document(doc):\n",
    "    title = doc[\"metadata\"][\"title\"]\n",
    "    abstract = \" \".join([paragraph[\"text\"] for paragraph in doc[\"abstract\"]])\n",
    "    text = \" \".join([paragraph[\"text\"] for paragraph in doc[\"body_text\"]])\n",
    "    returned = {}\n",
    "    returned[\"id\"] = doc[\"paper_id\"]\n",
    "    returned[\"title\"] = title\n",
    "    returned[\"abstract\"] = abstract\n",
    "    returned[\"body\"] = text\n",
    "    return returned\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path) as file:\n",
    "        file_json = json.load(file)\n",
    "    returned = load_document(file_json)\n",
    "    return returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is built (dataframe with paper_id, title, abstract and body columns for each document in \"path_texts\"). Then, the relevant parts of each row are selected (id and title)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.905Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(path_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.909Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2-3 minutes\n",
    "titles = dataset[[\"id\",\"title\"]].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.912Z"
    }
   },
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judgements\n",
    "\n",
    "The next file needed for the evaluation is the judgements. This file contains for each query a set of cord_uid (unique identifiers for each paper) and, linked with each pair (query, cord_uid), the score (relevance). The score can be 0,1 or 2 so, the first step is to binarize it (1 if the score is greater or equal to 1, and 0 otherwise). The documents selected for each query comes from the suggestions given by different search engines. With these documents, a \"pool\" is generated and experts analyze if the documents in the pool are relevant for the query or not. The content of judgements will be the key to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.915Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_judgements(path_judgements):\n",
    "    judgements = pd.read_csv(path_judgements, delimiter=' ', names = [\"query\", \"cord_uid\", \"score\"], usecols=[0,2,3])\n",
    "    judgements.loc[judgements['score'] < 1, 'binary_score'] = 0\n",
    "    judgements.loc[judgements['score'] >=1 , 'binary_score'] = 1\n",
    "    return judgements\n",
    "\n",
    "judgements = load_judgements(path_judgements2)\n",
    "judgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "The content of the file help to link the number of the query in the judgements file with the text of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.918Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_queries(queries_path):\n",
    "    \"\"\"\n",
    "    Receives the path of the queries files and returns a dictionary containing all the queries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    queries_path : path of the queries file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dic_judgements : dictionary\n",
    "\n",
    "    \"\"\"\n",
    "    with open(queries_path, \"r\") as xml_file:\n",
    "        data_dict = xtd.parse(xml_file.read())\n",
    "    xml_file.close()\n",
    "\n",
    "    dic_queries = {}\n",
    "    for query in data_dict[\"topics\"][\"topic\"]:\n",
    "        dic_queries[query[\"@number\"]] = query[\"query\"]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dic_queries, orient='index', columns=['query'])\n",
    "\n",
    "    return df\n",
    "\n",
    "queries = load_queries(path_queries)\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata file link the identifier of the paper used for indexing the documents with the cord_uid given in the judgements file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.922Z"
    }
   },
   "outputs": [],
   "source": [
    "#path_metadata = \"B:/document_parser/document_parses/metadata.csv\"\n",
    "path_metadata = \"C:/Users/David.Rubio/Desktop/muia/2020-07-16/metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.925Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(path_metadata, header = 0, usecols = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.927Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata = metadata.assign(sha=metadata.sha.str.split('; ')).explode('sha')\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.930Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata['sha'].isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.933Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata = metadata.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Model\n",
    "\n",
    "The TFIDF (Term Frequency Inversed Document Frequency) model is a numerical statistical model. This model returns a score which indicates how important is a word/query for a document in a corpus (collection). This score is proportional to the number of times the word(s) appears (tf) and inversely proportional to the number of documents that contains this word(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we need to do is loading the bag of words created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.943Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = corpora.MmCorpus('covid19.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we also load the dictionary and the similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.946Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load(\"covid19.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.949Z"
    }
   },
   "outputs": [],
   "source": [
    "similarities = Similarity.load(\"covid19.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we loaded the bow, dictionary and similarities, we are able to create our TFIDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.952Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1-2 minutes\n",
    "# Default model uses nfc from SMART notation.\n",
    "model_tfidf = models.TfidfModel(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to execute a query that returns the ranking of documents retrieved for that specific query. Moreover, a util function to preprocess the query is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.956Z"
    }
   },
   "outputs": [],
   "source": [
    "def launch_query_tfidf(model, dictionary, bow, index, query, titles, verbose = 0):\n",
    "    \"\"\"\n",
    "    Given a specific query, it returns the ranking of documents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tfidf model\n",
    "    dictionary: dictionary created\n",
    "    bow: bag of words\n",
    "    index: similarities matrix\n",
    "    query: specific query\n",
    "    titles: dataframe of the titles\n",
    "    verbose: flag for printing messages\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ranking: ranking for the query in this format: [(doc_position, score), (doc_position, score), ... ]\n",
    "    similarities: similarities matrix\n",
    "\n",
    "    \"\"\"\n",
    "    stopset = set(stopwords.words(\"english\"))\n",
    "    index = index\n",
    "    pq = preprocess_query_tfidf(query, stopset)\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = model[vq]\n",
    "    sim = index[qtfidf]\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "    if verbose:\n",
    "        print(\"Query ==> \"+query)\n",
    "        for doc, score in ranking[:5]:\n",
    "            print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + titles['title'].iloc[doc])\n",
    "    return ranking, sim\n",
    "        \n",
    "def preprocess_query_tfidf(query, stopset):\n",
    "    \"\"\"\n",
    "    Basic function that preprocess a query given a stopset: tokenization, lower case, stopwords removal and stemming.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query: query to preprocess\n",
    "    stopset: set containing stopwords\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    stems: stems of the given query\n",
    "    \n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = wordpunct_tokenize(query)\n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    "    stems = [stemmer.stem(word) for word in clean]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute it to show you how it works. (Notice that in this case verbose is marked as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.960Z"
    }
   },
   "outputs": [],
   "source": [
    "ranking, sim = launch_query_tfidf(model_tfidf, dictionary, bow, similarities, queries.iloc[1][0], titles, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of evaluation\n",
    "\n",
    "Once we are able to launch queries, it's time for evaluation. We are going to show you step by step the process of evaluation for the query launched before. Once this process is explained, we will create parameterized functions in order to improve the quality and organization of our code, making us able to reuse it later.\n",
    "\n",
    "The first thing that we do is creating a dataframe from the ranking obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.963Z"
    }
   },
   "outputs": [],
   "source": [
    "ranking = [[position[0], position[1]] for position in ranking]\n",
    "ranking = pd.DataFrame(ranking, columns=[\"doc_position\", \"rel_score\"])\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we include the sha identifier of each document result and its global ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.967Z"
    }
   },
   "outputs": [],
   "source": [
    "results = ranking\n",
    "results[\"sha\"] = results[\"doc_position\"].map(lambda x: titles[\"id\"].iloc[x])\n",
    "results[\"ranking\"] = results.index\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we join this dataframe with metadata in order to obtain the cord_uid of each document retrieved. This will allow us later to obtain the relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.971Z"
    }
   },
   "outputs": [],
   "source": [
    "results = results.set_index(\"sha\").join(metadata.set_index(\"sha\"))\n",
    "results = results.sort_values(by=[\"ranking\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that there are some null values in the cord_uid column. This means that some documents of our corpus don't have a cord_uid in the metadata, so we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.974Z"
    }
   },
   "outputs": [],
   "source": [
    "results['cord_uid'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.977Z"
    }
   },
   "outputs": [],
   "source": [
    "results = results.dropna()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we launched query 2, so now we need to obtain the relevance judgements of query 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.982Z"
    }
   },
   "outputs": [],
   "source": [
    "judgements_parcial = judgements[judgements[\"query\"] == 2]\n",
    "judgements_parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the judgements of query 2, we join this judgements with our ranking. This operation will allow us to know for each document retrieved whether it was relevant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.985Z"
    }
   },
   "outputs": [],
   "source": [
    "statistics = results.set_index(\"cord_uid\").join(judgements_parcial.set_index(\"cord_uid\"))\n",
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we observe that there are null values. These null values mean that there are some documents of our corpus that are not relevance judged. \n",
    "\n",
    "At this point have two possible options:\n",
    "* Drop the null values: since those documents are not judged, we don't know whether they are relevant or not for that query. It makes sense to us dropping those documents, since assuming that they are relevant or not introduces uncertainty in our model.\n",
    "* Mark them as non-relevant: after reading the TREC guide for evaluation, we understood how these relevance judgements files are created by means of pools. It is indicated that documents not included in the pool should be marked as non-relevant. However, it's also said that this approach is a bit controversial. \n",
    "\n",
    "After some more research, we realized that this is one of the most controversial aspects on evaluating information retrieval systems. Finally, we opted for the first action, since it made more sense for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop values\n",
    "statistics = statistics.dropna()\n",
    "# Mark them as non relevant\n",
    "#a[\"binary_score\"].fillna(0, inplace=True)\n",
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we already know whether each retrieved document is relevant or not.\n",
    "\n",
    "Now, we compute the total number of relevant documents, it will be used later.\n",
    "\n",
    "And, we remove all the results whose computed tfidf score is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.991Z"
    }
   },
   "outputs": [],
   "source": [
    "r = statistics[\"binary_score\"].sum()\n",
    "statistics = statistics[statistics[\"rel_score\"]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.994Z"
    }
   },
   "outputs": [],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We order our statistics by the ranking and reset the index.\n",
    "\n",
    "We compute the actual rank (starting in 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:55.998Z"
    }
   },
   "outputs": [],
   "source": [
    "statistics = statistics.sort_values(by=[\"ranking\"])\n",
    "statistics = statistics.reset_index()\n",
    "statistics[\"rank\"] = statistics.index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T17:05:40.594445Z",
     "start_time": "2020-12-07T17:05:40.591445Z"
    }
   },
   "source": [
    "Now, we compute precision and recall for each position of the ranking. Notice that we are making the table explained in the lecture notes (slide 51)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.003Z"
    }
   },
   "outputs": [],
   "source": [
    "statistics[\"relevant\"] = statistics[\"binary_score\"]\n",
    "statistics[\"rel_retrieved\"] = statistics[\"binary_score\"].cumsum()\n",
    "statistics[\"precision\"] = statistics[\"rel_retrieved\"] / statistics[\"rank\"]\n",
    "statistics[\"recall\"] = statistics[\"rel_retrieved\"] / r\n",
    "statistics.drop([\"doc_position\", \"score\", \"binary_score\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.005Z"
    }
   },
   "outputs": [],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compute the Mean Average Precision, the P@5 and P@10 and the R-Precision.\n",
    "\n",
    "These metrics and the precision and recall will be very helpful for the evaluation of our models.\n",
    "\n",
    "* **Precision** = relevant documents retrieved / documents retrieved. \n",
    "* **Recall** = relevant documents retrieved / total amount of relevant documents\n",
    "* **P@X**: precision at position X of our ranking\n",
    "* **R-Precision**: precision at position R of our ranking, being R the total number of relevant documents for that query (remember that before we computed some r? That r is this R.)\n",
    "* **Mean Average Precision**: average of precisions after some relevant document is retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.008Z"
    }
   },
   "outputs": [],
   "source": [
    "mavp = (statistics[\"precision\"] * statistics[\"relevant\"]).sum() / r\n",
    "r_precision = statistics.iloc[int(r-1)].precision\n",
    "p_5 = statistics.iloc[4].precision\n",
    "p_10 = statistics.iloc[9].precision\n",
    "\n",
    "print(f\"Mean average precision of TFIDF model for query 2 ==> {mavp}\")\n",
    "print(f\"R-precision of TFIDF model for query 2 ==> {r_precision}\")\n",
    "print(f\"P@5 of this model for query 2 ==> {p_5}\")\n",
    "print(f\"P@10 of this model for query 2 ==> {p_10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the precision vs recall figure, but using the standarized metric as explained in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.012Z"
    }
   },
   "outputs": [],
   "source": [
    "precision = statistics[\"precision\"].values\n",
    "recall = statistics[\"recall\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.014Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_higher_precision(statistics, recall):\n",
    "    \"\"\"\n",
    "    Auxiliary function to obtain the higher precision for a given recall value.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        index = statistics.index[statistics['recall'] >= recall].tolist()[0] \n",
    "        return statistics.iloc[index:][\"precision\"].max()\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.017Z"
    }
   },
   "outputs": [],
   "source": [
    "precision_recall = [[recall_i, get_higher_precision(statistics, recall_i)] for recall_i in np.arange(0,recall.max()+0.1, 0.1)]\n",
    "precision = [precision[1] for precision in precision_recall]\n",
    "recall = [recall[0] for recall in precision_recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.019Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.title(\"Precision vs recall standardized\")\n",
    "plt.plot(recall, precision, \"r-o\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterized Evaluation\n",
    "\n",
    "Before, we have explained step by step how the statistics are computed.\n",
    "\n",
    "The following three cells have the same code as above, but in functions so as to be reused later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.021Z"
    }
   },
   "outputs": [],
   "source": [
    "def launch(function, model,dictionary,bow,index,titles):\n",
    "    \"\"\"\n",
    "    God function that allow us to pass independent launch query functions to our get_statistics function. \n",
    "    For more info, check Currying (Functional Programming).\n",
    "    \n",
    "    \"\"\"\n",
    "    def h(query):\n",
    "        return function(model,dictionary,bow,index,query,titles)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.024Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_statistics(launch_query, queries, titles, metadata, judgements):\n",
    "    \"\"\"\n",
    "    General function that allow us to get statistics for all queries given a specific launch query function (model).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    It returns all the metrics explained before.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for index in range(len(queries)):\n",
    "        ranking, _ = launch_query(queries.iloc[index][0])#\n",
    "        result = get_statistics_query(ranking, index+1, titles, metadata, judgements)\n",
    "        results.append(result)\n",
    "    mavp = np.array([query[\"mavp\"] for query in results]).mean()\n",
    "    precision = np.array([query[\"precision\"] for query in results]).mean(axis=0)\n",
    "    r_precision = [query[\"r-precision\"] for query in results]\n",
    "    p_5 = [query[\"p_5\"] for query in results]\n",
    "    p_10 = [query[\"p_10\"] for query in results]\n",
    "    recall = np.array([query[\"recall\"] for query in results]).mean(axis=0)\n",
    "    return {\"mavp\": mavp, \"precision\":precision, \"r-precision\":r_precision, \"recall\":recall, \"p_5\": p_5, \"p_10\": p_10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.026Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_statistics_query(ranking, id_query, titles, metadata, judgements):\n",
    "    \"\"\"\n",
    "    Individual function to compute the statistics for a specific query.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    It returns all the metrics explained before.\n",
    "    \"\"\"\n",
    "    ranking = [[position[0], position[1]] for position in ranking]\n",
    "    ranking = pd.DataFrame(ranking, columns=[\"doc_position\", \"rel_score\"])\n",
    "    results = ranking\n",
    "    results[\"sha\"] = results[\"doc_position\"].map(lambda x: titles[\"id\"].iloc[x])\n",
    "    results[\"ranking\"] = results.index\n",
    "    results = results.set_index(\"sha\").join(metadata.set_index(\"sha\"))\n",
    "    results = results.sort_values(by=[\"ranking\"])\n",
    "    results = results.dropna()\n",
    "    judgements_parcial = judgements[judgements[\"query\"] == id_query]\n",
    "    statistics = results.set_index(\"cord_uid\").join(judgements_parcial.set_index(\"cord_uid\"))\n",
    "    statistics = statistics.dropna()\n",
    "    r = statistics[\"binary_score\"].sum()\n",
    "    statistics = statistics[statistics[\"rel_score\"]>0]\n",
    "    statistics = statistics.sort_values(by=[\"ranking\"])\n",
    "    statistics = statistics.reset_index()\n",
    "    statistics[\"rank\"] = statistics.index+1\n",
    "    statistics[\"relevant\"] = statistics[\"binary_score\"]\n",
    "    statistics[\"rel_retrieved\"] = statistics[\"binary_score\"].cumsum()\n",
    "    statistics[\"precision\"] = statistics[\"rel_retrieved\"] / statistics[\"rank\"]\n",
    "    r = statistics[\"relevant\"].sum()\n",
    "    if r == 0:\n",
    "        return {\"mavp\": 0, \"r-precision\": 0, \"precision\": list(np.zeros(11)), \"recall\":0, \"p_10\": 0, \"p_5\": 0}\n",
    "    statistics[\"recall\"] = statistics[\"rel_retrieved\"] / r\n",
    "    statistics.drop([\"doc_position\", \"score\", \"binary_score\"], axis=1, inplace=True)\n",
    "    mavp = (statistics[\"precision\"] * statistics[\"relevant\"]).sum() / r\n",
    "    r_precision = statistics.iloc[int(r-1)].precision\n",
    "    precision = statistics[\"precision\"].values\n",
    "    recall = statistics[\"recall\"].values\n",
    "    recall_range = np.arange(0,recall.max()+0.1, 0.1)\n",
    "    precision_recall = [[recall, get_higher_precision(statistics, recall)] for recall in recall_range]\n",
    "    precision = [precision[1] for precision in precision_recall]\n",
    "    p_5 = statistics.iloc[4].precision\n",
    "    p_10 = statistics.iloc[9].precision\n",
    "    return {\"mavp\": mavp, \"r-precision\": r_precision, \"precision\": precision, \"recall\":recall_range, \"p_10\": p_10, \"p_5\":p_5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF 2.0\n",
    "\n",
    "Once we have all our structure created: launching queries and computing statistics, we create 4 variations of the TFIDF Model playing with the SMART notation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.030Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4-6 minutes\n",
    "model_tfidf_3 = models.TfidfModel(bow, smartirs=\"lpu\") # Logarithmic term frequency weighting, probabilistic document frequency weighting \n",
    "                                                        # and pivoted unique normalization\n",
    "model_tfidf_2 = models.TfidfModel(bow, smartirs=\"lfc\") # Logarithmic term frequency weighting, idf document frequency weighting \n",
    "                                                         # and cosine normalization\n",
    "model_tfidf_5 = models.TfidfModel(bow, smartirs=\"ltc\") # Logarithmic term frequency weighting, zero-corrected idf document frequency weighting \n",
    "                                                         # and cosine normalization\n",
    "model_tfidf_4 = models.TfidfModel(bow, smartirs=\"nnn\") # Raw term frequency weighting, none document frequency weighting \n",
    "                                                         # and none normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "After creating several TFIDF models, we wanted to try the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.033Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\&)|(\\%)|(\\$)|(\\€)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\⁰)|(\\•)|(\\\\')\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def clean(doc):\n",
    "    \"\"\"\n",
    "    Basic function to clean a document being a document a string with words separated by blank spaces.\n",
    "    \"\"\"\n",
    "    doc = REPLACE_NO_SPACE.sub(NO_SPACE, doc.lower())\n",
    "    doc = REPLACE_WITH_SPACE.sub(SPACE, doc)\n",
    "    stopset = set(stopwords.words(\"english\"))\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token.lower() for token in tokens if token not in stopset and len(token) > 2 and not token.isnumeric()]\n",
    "    final = [stemmer.stem(token) for token in clean]\n",
    "    return \" \".join(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation\n",
    "\n",
    "The first thing that we need to do is to create our Word2Vec model. To do that, we need to obtain our dataset (documents and queries).\n",
    "\n",
    "The following cells create the dataset for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.037Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = pd.DataFrame((line for line in open(\"docs.txt\")), columns = [\"preprocess\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.040Z"
    }
   },
   "outputs": [],
   "source": [
    "queries_w2v = queries.copy()\n",
    "queries_w2v[\"preprocess\"] = queries_w2v[\"query\"].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.042Z"
    }
   },
   "outputs": [],
   "source": [
    "combined = pd.concat((docs[\"preprocess\"], queries_w2v[\"preprocess\"])).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our dataset, it´s time to create and train our model. \n",
    "* **Size**: this is the size of the word embeddings. We chose this value after looking in some papers that suggested using a size between 200-500 for large corpus (as ours).\n",
    "* **Workers**: if you execute this code, please change this number to maximum number of threads of your computer (cores x 2 if you have hyper threading)\n",
    "\n",
    "**IMPORTANT** The following cell takes 10 minutes to finish in a computer with a Ryzen 5 3600. Remember this before executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.046Z"
    }
   },
   "outputs": [],
   "source": [
    "# 10-20 minutes\n",
    "w2v_model = models.Word2Vec([i.split() for i in combined], size = 300, workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.050Z"
    }
   },
   "outputs": [],
   "source": [
    "len(w2v_model.wv.vocab) # We check the size of our vocabulary, having more than 300k unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving\n",
    "\n",
    "After creating our model, we can save it for later use in other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.056Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "word_vectors.save(\"word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.058Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.061Z"
    }
   },
   "outputs": [],
   "source": [
    "del w2v_model # We delete the model since we only need the word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "\n",
    "We load our word_vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.066Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.069Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding_w2v(doc_tokens):\n",
    "    \"\"\"\n",
    "    Basic function to obtain the vector representation of a given document (list of tokens).\n",
    "    \n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    if len(doc_tokens) < 1: # If the length of our document is 0 we return a vector of zeros with size 300 (remember this is the vector size used when creating the model)\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        for token in doc_tokens:\n",
    "            if token in word_vectors.vocab: # if the word is in our vocabulary, we get its vector\n",
    "                embeddings.append(word_vectors.word_vec(token))\n",
    "            else:\n",
    "                embeddings.append(np.random.rand(300)) # if it´s not in our vocabulary a random vector is used\n",
    "        return np.mean(embeddings, axis = 0) # the vector representation of a document is the mean of its word´s vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries embeddings\n",
    "\n",
    "We compute the vector representations of our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.072Z"
    }
   },
   "outputs": [],
   "source": [
    "queries_w2v[\"embeddings\"] = queries_w2v[\"preprocess\"].apply(lambda x: get_embedding_w2v(x.split()))\n",
    "queries_w2v.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the amount of embeddings to be computed, we filter the documents and we only save the ones that have some query performed on it and appear in metadata.\n",
    "Then, we compute the embedding for each document, so as to have it precomputed and then, when computing the similarity, just taking this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.077Z"
    }
   },
   "outputs": [],
   "source": [
    "titles_w2v = titles.copy()\n",
    "titles_w2v = titles_w2v.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.080Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = titles_w2v.join(docs)\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.082Z"
    }
   },
   "outputs": [],
   "source": [
    "# With this filter, we only use documents that appear in both judgements and metadata files, i.e, documents that have queries related.\n",
    "docs[\"sha\"] = docs[\"id\"]\n",
    "inCorpus = metadata[metadata[\"sha\"].isin(docs[\"sha\"])][\"cord_uid\"]\n",
    "inJudgements = inCorpus[inCorpus.isin(judgements[\"cord_uid\"])]\n",
    "filtered = metadata.loc[metadata['cord_uid'].isin(inJudgements)][\"sha\"].values\n",
    "docs_filtered = docs.loc[docs[\"id\"].isin(filtered)]\n",
    "docs_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.085Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5-10 minutes\n",
    "docs_filtered[\"embeddings\"] = docs_filtered[\"preprocess\"].apply(lambda x: get_embedding_w2v(x.split())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.087Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch query in W2V Model\n",
    "\n",
    "As before with the TFIDF model, we create two functions to launch queries with our W2V model.\n",
    "\n",
    "We will show you the results for query 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.091Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "i = 2\n",
    "\n",
    "def launch_query_w2v(query_name, queries, docs_filtered, verbose=0):\n",
    "    \"\"\"\n",
    "    General function that allow us to launch queries to our W2V model.\n",
    "    \n",
    "    Parameter:\n",
    "    query_name: the name of the query\n",
    "    queries: dataframe of the queries\n",
    "    docs_filtered: dataframe of filtered documents with their vector representation (the one explained before)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    It returns the ranking.\n",
    "    \n",
    "    \"\"\"\n",
    "    query_name = query_name\n",
    "    # We obtain the vector of the query\n",
    "    query_vector = queries[queries[\"query\"] == query_name][\"embeddings\"].values[0]\n",
    "    docs_aux = docs_filtered.copy()\n",
    "    # We compute the cosine similarity of the query´s vector with the documents vectors\n",
    "    docs_aux[\"score\"] = docs_aux[\"embeddings\"].apply(\n",
    "        lambda x: cosine_similarity(np.array(query_vector).reshape(1,-1), np.array(x).reshape(1,-1)).item())\n",
    "    # We order the docs by this similarity\n",
    "    docs_aux = docs_aux.sort_values(by=\"score\", ascending=False)\n",
    "    doc_pos = docs_aux.index.values\n",
    "    rel_score = docs_aux[\"score\"].values\n",
    "    ranking = [z for z in zip(doc_pos, rel_score)]\n",
    "    if verbose:\n",
    "        print(\"Query ==> \"+query_name)\n",
    "        for i in range(5):\n",
    "            score = docs_aux.iloc[i][\"score\"]\n",
    "            title = docs_aux.iloc[i][\"title\"] if docs_aux.iloc[i][\"title\"]!=\"\" else \"No title available\"\n",
    "            print(f\"[Score = {score}] \"+ title)\n",
    "    return ranking, \"Just to work\"\n",
    "\n",
    "def launch_w2v(function, queries, docs_filtered):\n",
    "    \"\"\"\n",
    "    Again, function that allow us to reuse our code for evaluation.\n",
    "    \"\"\"\n",
    "    def h(query_name):\n",
    "        return function(query_name, queries, docs_filtered)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution of query 2 with its results. Notice that now the scores are higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.094Z"
    }
   },
   "outputs": [],
   "source": [
    "ranking, _ = launch_query_w2v(queries_w2v.iloc[1][0], queries_w2v, docs_filtered, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast text is a model based on the w2v model and the key difference among them is the use of n-grams. w2v treats each word as an atomic entity, so it generates a vector for each word. On the other hand, fastText considers each word as composed of character n-grams (the vector of the word is composed by the sum of its character n-grams - e.g. apple with smallest n-gram of 3 and largest n-gram of 6: ['<ap','app','appl','apple','apple>','ppl','pple','pple>','ple','ple>','le>']). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation\n",
    "\n",
    "In order to create the model, we use the same dataset as for w2v.\n",
    "\n",
    "**IMPORTANT** The fast text model takes around 30 minutes to be computed, be careful before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.099Z"
    }
   },
   "outputs": [],
   "source": [
    "# 20-30 minutes\n",
    "fasttext_model = models.FastText([i.split() for i in combined], size = 50, workers=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving\n",
    "\n",
    "After creating our model, we can save it for later use in other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.103Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors_ft = fasttext_model.wv\n",
    "word_vectors_ft.save(\"fasttext_model.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.107Z"
    }
   },
   "outputs": [],
   "source": [
    "fasttext_model.save(\"fasttext_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delete the model since we only need the word_vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.110Z"
    }
   },
   "outputs": [],
   "source": [
    "del fasttext_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "\n",
    "We load our word_vectors for the fast text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.113Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors_ft = KeyedVectors.load(\"fasttext_model.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting embeddings\n",
    "\n",
    "The method *get_embedding_ft* obtain the vector representation of a given document. This document is represented as a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.117Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding_ft(doc_tokens):\n",
    "    \"\"\"\n",
    "    Basic function to obtain the vector representation of a given document (list of tokens).\n",
    "    \n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    if len(doc_tokens) < 1: # If the length of our document is 0 we return a vector of zeros with size 300 (remember this is the vector size used when creating the model)\n",
    "        return np.zeros(50)\n",
    "    else:\n",
    "        for token in doc_tokens:\n",
    "            if token in word_vectors_ft.vocab: # if the word is in our vocabulary, we get its vector\n",
    "                embeddings.append(word_vectors_ft.word_vec(token))\n",
    "            else:\n",
    "                embeddings.append(np.random.rand(50))\n",
    "                #key = word_vectors_ft.most_similar(token)[0][0]\n",
    "                #embeddings.append(word_vectors_ft.word_vec(key)) # if it´s not in our vocabulary a random vector is used\n",
    "        return np.mean(embeddings, axis = 0) # the vector representation of a document is the mean of its word´s vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries embeddings\n",
    "\n",
    "We compute the vector representations of our queries (represented as a list of tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.120Z"
    }
   },
   "outputs": [],
   "source": [
    "queries_ft = queries.copy()\n",
    "queries_ft[\"preprocess\"] = queries_ft[\"query\"].apply(lambda x: clean(x))\n",
    "queries_ft[\"embeddings\"] = queries_ft[\"preprocess\"].apply(lambda x: get_embedding_ft(x.split()))\n",
    "queries_ft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents embeddings\n",
    "\n",
    "As in w2v, in order to reduce the amount of embeddings to be computed, we filter the documents and we only save the ones that have some query performed on it and appear in metadata. Then, we compute the embedding for each document, so as to have it precomputed and then, when computing the similarity, just taking this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.123Z"
    }
   },
   "outputs": [],
   "source": [
    "titles_ft = titles.copy()\n",
    "titles_ft = titles_ft.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.127Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = pd.DataFrame((line for line in open(\"docs.txt\")), columns = [\"preprocess\"])\n",
    "docs_ft = titles_ft.join(docs)\n",
    "docs_ft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.132Z"
    }
   },
   "outputs": [],
   "source": [
    "# With this filter, we only use documents that appear in both judgements and metadata files, i.e, documents that have queries related.\n",
    "docs_ft[\"sha\"] = docs_ft[\"id\"]\n",
    "inCorpus_ft = metadata[metadata[\"sha\"].isin(docs_ft[\"sha\"])][\"cord_uid\"]\n",
    "inJudgements_tf = inCorpus_ft[inCorpus_ft.isin(judgements[\"cord_uid\"])]\n",
    "filtered_ft = metadata.loc[metadata['cord_uid'].isin(inJudgements_tf)][\"sha\"].values\n",
    "docs_filtered_ft = docs_ft.loc[docs_ft[\"id\"].isin(filtered_ft)]\n",
    "docs_filtered_ft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.135Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5-10 minutes\n",
    "docs_filtered_ft[\"embeddings\"] = docs_filtered_ft[\"preprocess\"].apply(lambda x: get_embedding_ft(x.split())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.137Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_filtered_ft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch query in FastText Model\n",
    "\n",
    "As in w2v, we need to create two functions to launch queries with out FastText model. However, these functions are the same as in w2c so we will reuse them.\n",
    "\n",
    "Execution of query 2 with its results. Notice that now the scores are higher than in TFIDF and w2v models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.140Z"
    }
   },
   "outputs": [],
   "source": [
    "ranking_ft, _ = launch_query_w2v(queries_ft.iloc[1][0], queries_ft, docs_filtered_ft, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI (Deerwester et al., 1990) is a retrieval model based on distributional semantics, that assumes that words tend to have similar meanings if they are used in the same context. This model uses Singular Value Decomposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we first create the model. To do that, we need to compute the corpus in tfidf format. Once it's done, we create the model using 100 as number of topics. The related literature suggested using a num_topics from hundreds to thousand, so we chose 100 since for this value we already obtain good performance and the execution time isn't too big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** The following cell takes 5 minutes more or less to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.145Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7-10 minutes\n",
    "corpus_tfidf = model_tfidf[bow]\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics = 100)\n",
    "lsi.save(\"model_lsi.lsi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.148Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3-5 minutes\n",
    "index_lsi = Similarity(None, lsi[corpus_tfidf], num_features=len(dictionary))\n",
    "index_lsi.save(\"covid19-lsi.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the code needed to launch a query with LSI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.152Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def launch_query_lsi(model, dictionary, bow, index, query, titles, verbose = 0):\n",
    "    \"\"\"\n",
    "    Given a specific query, it returns the ranking of documents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tfidf model\n",
    "    dictionary: dictionary created\n",
    "    bow: bag of words\n",
    "    index: similarities matrix\n",
    "    query: specific query\n",
    "    titles: dataframe of the titles\n",
    "    verbose: flag for printing messages\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ranking: ranking for the query in this format: [(doc_position, score), (doc_position, score), ... ]\n",
    "    similarities: similarities matrix\n",
    "\n",
    "    \"\"\"\n",
    "    stopset = set(stopwords.words(\"english\"))\n",
    "    index = index\n",
    "    pq = preprocess_query_tfidf(query, stopset)\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    veclsi = model[model_tfidf[vq]]\n",
    "    sim = index[veclsi]\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "    if verbose:\n",
    "        print(\"Query ==> \"+query)\n",
    "        for doc, score in ranking[:5]:\n",
    "            print(\"[ Score = \" + \"%.3f\" % round(score,3) + \" ] \" + titles['title'].iloc[doc])\n",
    "    return ranking, sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching query *origin coronavirus*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.156Z"
    }
   },
   "outputs": [],
   "source": [
    "ranking, _ = launch_query_lsi(lsi, dictionary, bow, index_lsi, \"origin coronavirus\", titles, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "So, after explaining how we implemented the models, it's turn to evaluate them.\n",
    "\n",
    "We will consider the metrics explained before: precision, recall, mean average precision, P@5, P@10 and R-Precision.\n",
    "\n",
    "The following cell performs all the evaluation:\n",
    "* We print the P@5, P@10 and MAVP of each single document.\n",
    "* We plot a graph comparing the precision and recall of all the models tested.\n",
    "* We plot a graph comparing the difference of the R-precision of the best two models for each single query. If the values are positive mean that R-precision of model A was greater than model's B R-Precision. Negative values mean that that R-precision of model B was greater than model's A R-Precision.\n",
    "\n",
    "The evaluation procedure was created following the indications in (Baeza and Ribeiro, 2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T19:54:56.160Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 13-20 minutes\n",
    "recall = np.arange(0,1.1, 0.1)\n",
    "modelos_tfidf = [{\"name\":\"TFIDF (nfc)\", \"term\": \"raw\", \"document\": \"idf\", \"normalization\": \"cosine\", \"model\": model_tfidf, \"color\": \"r\"},\n",
    "          {\"name\":\"TFIDF (lfc)\", \"term\": \"logarithmic\", \"document\": \"idf\", \"normalization\": \"cosine\", \n",
    "           \"model\": model_tfidf_2, \"color\": \"g\"},\n",
    "           {\"name\":\"TFIDF (lpu)\", \"term\": \"logarithmic\", \"document\": \"probabilistic\", \"normalization\": \"pivoted unique\", \n",
    "           \"model\": model_tfidf_3, \"color\": \"b\"},\n",
    "           {\"name\":\"TFIDF (nnn)\", \"term\": \"raw\", \"document\": \"none\", \"normalization\": \"none\", \n",
    "           \"model\": model_tfidf_4, \"color\": \"c\"},\n",
    "           {\"name\":\"TFIDF (ltc)\", \"term\": \"logarithmic\", \"document\": \"zero-corrected idf\", \"normalization\": \"cosine\", \n",
    "           \"model\": model_tfidf_5, \"color\": \"m\"}\n",
    "         ]\n",
    "\n",
    "modelos_w2v = [{\"name\": \"Word2Vec\", \"algorithm\": \"Continuous Bag of Words\", \"vector\": 300, \n",
    "                \"window\": \"5 (default)\", \"negative\": \"5 (default)\", \"epochs\": \"5 (default)\", \"color\":\"orange\"}\n",
    "              ]\n",
    "\n",
    "modelos_ft = [{\"name\":\"FastText\",  \"algorithm\": \"Continuous Bag of Words\", \"vector\": 50, \n",
    "                \"window\": \"5 (default)\", \"negative\": \"5 (default)\", \"epochs\": \"5 (default)\", \"color\":\"y\"}]\n",
    "\n",
    "modelos_lsi = [{\"name\": \"Latent Semantic Index\", \"vector\": 100, \"version\": \"tfidf\", \"color\":\"k\", \"model\": lsi}]\n",
    "               \n",
    "r_precisions = []\n",
    "precisions_5 = []\n",
    "precisions_10 = []\n",
    "\n",
    "print(\"###################################################################################\")\n",
    "print(\"Evaluating performance of our models...\")\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title(\"Precision vs Recall standardized\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "\n",
    "for model in modelos_tfidf:\n",
    "    name = model[\"name\"]\n",
    "    term = model[\"term\"]\n",
    "    document = model[\"document\"]\n",
    "    normalization = model[\"normalization\"]\n",
    "    color = model[\"color\"]\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"Evaluating {name}: {term} term frequency weighing, {document} document frequency weighting, {normalization} normalization\")\n",
    "    print(\"Computing statistics...\")\n",
    "    data_tfidf = get_statistics(launch(launch_query_tfidf,model[\"model\"], dictionary, bow, similarities, titles.copy()), queries.copy(), titles.copy(), metadata.copy(), judgements.copy())\n",
    "    print(\"Statistics computed...\")\n",
    "    precision_tfidf = data_tfidf[\"precision\"]\n",
    "    recall = data_tfidf[\"recall\"]\n",
    "    mavp_tfidf = data_tfidf[\"mavp\"]\n",
    "    precisions_5.append(np.array(data_tfidf[\"p_5\"]).mean())\n",
    "    precisions_10.append(np.array(data_tfidf[\"p_10\"]).mean())\n",
    "    r_precisions.append(data_tfidf[\"r-precision\"])\n",
    "    print(f\"Mean Average Precision of this model ==> {mavp_tfidf}\")\n",
    "    p_5 = np.array(data_tfidf[\"p_5\"]).mean()\n",
    "    p_10 = np.array(data_tfidf[\"p_10\"]).mean()\n",
    "    print(f\"Mean P@5 of this model ==> {p_5}\")\n",
    "    print(f\"Mean P@10 of this model ==> {p_10}\")\n",
    "\n",
    "    plt.plot(recall, precision_tfidf, f\"{color}-o\", label=name)\n",
    "\n",
    "    \n",
    "for model in modelos_w2v:\n",
    "    name = model[\"name\"]\n",
    "    vector = model[\"vector\"]\n",
    "    window = model[\"window\"]\n",
    "    negative = model[\"negative\"]\n",
    "    algorithm = model[\"algorithm\"]\n",
    "    epochs = model[\"epochs\"]\n",
    "    color = model[\"color\"]\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"Evaluating {name}: \\n\\tTraining algorithm = {algorithm}\\n\\tVector size = {vector}\\n\\tDistance predicted and current word = {window}\\n\\tNegative sampling = {negative}\\n\\tEpochs = {epochs}\")\n",
    "    print(\"Computing statistics...\")\n",
    "    data_w2v = get_statistics(launch_w2v(launch_query_w2v, queries_w2v.copy(), docs_filtered.copy()), queries_w2v.copy(), titles_w2v.copy(), metadata.copy(), judgements.copy())\n",
    "    print(\"Statistics computed...\")\n",
    "    precision_w2v = data_w2v[\"precision\"]\n",
    "    recall = data_w2v[\"recall\"]\n",
    "    mavp_w2v = data_w2v[\"mavp\"]\n",
    "    r_precisions.append(data_w2v[\"r-precision\"])\n",
    "    precisions_5.append(np.array(data_w2v[\"p_5\"]).mean())\n",
    "    precisions_10.append(np.array(data_w2v[\"p_10\"]).mean())\n",
    "    print(f\"Mean Average Precision of this model ==> {mavp_w2v}\")\n",
    "    p_5 = np.array(data_w2v[\"p_5\"]).mean()\n",
    "    p_10 = np.array(data_w2v[\"p_10\"]).mean()\n",
    "    print(f\"Mean P@5 of this model ==> {p_5}\")\n",
    "    print(f\"Mean P@10 of this model ==> {p_10}\")\n",
    "\n",
    "    plt.plot(recall, precision_w2v, \"-o\", color = color, label=name)\n",
    "\n",
    "    \n",
    "for model in modelos_ft:\n",
    "    name = model[\"name\"]\n",
    "    vector = model[\"vector\"]\n",
    "    window = model[\"window\"]\n",
    "    negative = model[\"negative\"]\n",
    "    algorithm = model[\"algorithm\"]\n",
    "    epochs = model[\"epochs\"]\n",
    "    color = model[\"color\"]\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"Evaluating {name}: \\n\\tTraining algorithm = {algorithm}\\n\\tVector size = {vector}\\n\\tDistance predicted and current word = {window}\\n\\tNegative sampling = {negative}\\n\\tEpochs = {epochs}\")\n",
    "    print(\"Computing statistics...\")\n",
    "    data_ft = get_statistics(launch_w2v(launch_query_w2v, queries_ft.copy(), docs_filtered_ft.copy()), queries_ft.copy(), titles_ft.copy(), metadata.copy(), judgements.copy())\n",
    "    print(\"Statistics computed...\")\n",
    "    precision_ft = data_ft[\"precision\"]\n",
    "    recall = data_ft[\"recall\"]\n",
    "    mavp_ft = data_ft[\"mavp\"]\n",
    "    r_precisions.append(data_ft[\"r-precision\"])\n",
    "    precisions_5.append(np.array(data_ft[\"p_5\"]).mean())\n",
    "    precisions_10.append(np.array(data_ft[\"p_10\"]).mean())\n",
    "    print(f\"Mean Average Precision of this model ==> {mavp_ft}\")\n",
    "    p_5 = np.array(data_ft[\"p_5\"]).mean()\n",
    "    p_10 = np.array(data_ft[\"p_10\"]).mean()\n",
    "    print(f\"Mean P@5 of this model ==> {p_5}\")\n",
    "    print(f\"Mean P@10 of this model ==> {p_10}\")\n",
    "\n",
    "    plt.plot(recall, precision_ft, f\"{color}-o\", label=name)\n",
    "    \n",
    "for model in modelos_lsi:\n",
    "    name = model[\"name\"]\n",
    "    vector = model[\"vector\"]\n",
    "    version = model[\"version\"]\n",
    "    color = model[\"color\"]\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(f\"Evaluating {name}: \\n\\tVector size = {vector}\\n\\tCorpus version = {version}\")\n",
    "    print(\"Computing statistics...\")\n",
    "    data_lsi = get_statistics(launch(launch_query_lsi,model[\"model\"], dictionary, model_tfidf[bow], index_lsi, titles.copy()), queries.copy(), titles.copy(), metadata.copy(), judgements.copy())\n",
    "    print(\"Statistics computed...\")\n",
    "    precision_lsi = data_lsi[\"precision\"]\n",
    "    recall = data_lsi[\"recall\"]\n",
    "    mavp_lsi = data_lsi[\"mavp\"]\n",
    "    r_precisions.append(data_lsi[\"r-precision\"])\n",
    "    precisions_5.append(np.array(data_lsi[\"p_5\"]).mean())\n",
    "    precisions_10.append(np.array(data_lsi[\"p_10\"]).mean())\n",
    "    print(f\"Mean Average Precision of this model ==> {mavp_lsi}\")\n",
    "    p_5 = np.array(data_lsi[\"p_5\"]).mean()\n",
    "    p_10 = np.array(data_lsi[\"p_10\"]).mean()\n",
    "    print(f\"Mean P@5 of this model ==> {p_5}\")\n",
    "    print(f\"Mean P@10 of this model ==> {p_10}\")\n",
    "\n",
    "    plt.plot(recall, precision_lsi, f\"{color}-o\", label=name)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Comparing our models: W2V vs LSI\")\n",
    "r_precision = np.array(r_precisions[-3]) - np.array(r_precisions[-1])\n",
    "# the histogram of the data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(np.arange(1,51,1),r_precision)\n",
    "plt.xlabel('Query number')\n",
    "plt.ylabel('R-Precision W2V/LSI')\n",
    "plt.title('Comparison of R-Precisions')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Evalution finished.\")\n",
    "print(\"###################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work aims to retrieve relevant COVID documents given a query and, hence, different models have been implemented and tested. The characteristics of these models are the following:\n",
    "\n",
    "* TFIDF. The term frequency (tf), document frequency (idf) and document normalization can be computed using different metrics. In this work, different combinations have been tested and the one with better performance is lpu: logarithm term frequency weighting, probabilist document frequency weighting and pivoted unique document normalization.\n",
    "* w2v (word2vec). This model computes the weights of the first layer of a neural network with one unique hiden layer. It performs better than any of the TFIDF models (r-precision, P@5 and P@10 improve their values).\n",
    "* fastText. It is based on the w2v model and the key difference among them is the use of n-grams. This model performs worse than w2v and take more time to train the model. Consequently, if the system will run in a real environment (not a learning one), we will exclude it from the possible retrieve models for the search engine. However, recent literature (Tiun et al.) reports that fastTest has better performance than w2v. This let us conclude that the performance of each model varies significantly among corpora. In comparison with TFIDF, we think that the improvement in the evaluation metrics is not worth it compared to the model training time.\n",
    "* Latent Semantic Indexing (LSI). This model considers the semantic meaning of the words: words that are in the same context tend to appear together. Regarding the mean average precision and the r-precisions, this model performs better than w2v. However, w2v achieves a higher value for P@5 and P@10. \n",
    "\n",
    "The two models that have a better performance are w2v and LSI. If we observe the graphic that compares the r-precision of both models in all models, we can see that, in general, LSI is better. However, P@5 and P@10 are higher in w2c and this suggest:\n",
    "\n",
    "* If we want to retrieve just a few results: better performance w2v (a higher value for P@5 and P@10).\n",
    "* If we want to retrieve all the results in the ranking: better performance LSI (a higher value for mean average precision). \n",
    "\n",
    "To conclude, we want to mention that there isn't any perfect model and, in this case, we select LSI and w2v. As a future line of work, we have seen in literature that results from different models can be integrated to retrieve a combined ranking. Additionally, doc2vec model will be tested in order to evaluate its performance: there is no need in getting the embeddings (reduces computational load)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T14:51:15.363156Z",
     "start_time": "2020-12-12T14:51:15.357158Z"
    }
   },
   "source": [
    "Baeza-Yates, R., & Ribeiro-Neto, B. (2011). Modern information retrieval: the concepts and technology behind search. Choice Reviews Online, 48(12), 48-6950-48–6950. https://doi.org/10.5860/choice.48-6950\n",
    "\n",
    "Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). Indexing by latent semantic analysis. In Journal of the American Society for Information Science (Vol. 41, Issue 6). https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9\n",
    "\n",
    "Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). Distributed representations ofwords and phrases and their compositionality. Advances in Neural Information Processing Systems. http://arxiv.org/abs/1310.4546\n",
    "\n",
    "Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. In 1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings. http://ronan.collobert.com/senna/\n",
    "\n",
    "Tiun, S., U. A. Mokhtar, S. H. Bakar, and S. Saad. (2020). Classification of functional and non-functional requirement in software requirement using Word2vec and fast Text. In Journal of Physics: Conference Series, vol. 1529, no. 4, p. 042077. https://doi.org/10.1088%2F1742-6596%2F1529%2F4%2F042077"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "322.5px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
